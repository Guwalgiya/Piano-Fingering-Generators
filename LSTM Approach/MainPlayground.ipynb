{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import system dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "import shutil\n",
    "import statistics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import custom packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Utils import shuffleDataset, SplitJPData, saveToPickle, loadFromPickle, SplitChopinData, SplitMozartData, SplitBachData\n",
    "from JPDataPreProcessing import toVectorTrainFormat, toVectorTestFormat, toInterleavedTrainFormat, toVectorFutureTrainFormat\n",
    "from TrainModel import trainModel\n",
    "from TestVectorModel import loadModel, testVecModelSave, testVecModelEval\n",
    "from EvaluateJPMethod import getScoresForDL, getScoresForHmm\n",
    "\n",
    "from ReferenceHMM import GetESTFingering\n",
    "from parameters import DATA_DIR, HMM_RES_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change parameters here\n",
    "composer  = 'bach'\n",
    "\n",
    "hmm_level = 'FHMM{}'.format(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Dataset According to Composer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if composer == 'Bach':\n",
    "    train_files, test_files, hmm_res_files = SplitBachData(DATA_DIR)\n",
    "elif composer == 'Mozart':\n",
    "    train_files, test_files, hmm_res_files = SplitMozartData(DATA_DIR)\n",
    "elif composer == 'Chopin':\n",
    "    train_files, test_files, hmm_res_files = SplitChopinData(DATA_DIR)\n",
    "else:\n",
    "    raise Exception('Wrong Composer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train all 3 model of reference HMM with the data splited above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../ReferenceHMM')\n",
    "GetESTFingering.prepareInputList(train_files)\n",
    "GetESTFingering.trainHmm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run trained HMM model, change GetESTFingering.FHMMx to swith models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_dir = os.path.join('ESTResults/selfTrained',   hmm_level)\n",
    "csv_dir = os.path.join('JPESTResults/selfTrained', hmm_level)\n",
    "\n",
    "if not os.path.exists(txt_dir):\n",
    "    os.makedirs(txt_dir)\n",
    "\n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)\n",
    "    \n",
    "test_filenames = GetESTFingering.getFormattedTestFilenames(test_files)\n",
    "GetESTFingering.runHmm(test_filenames, hmm_level, default=False)\n",
    "\n",
    "GetESTFingering.convertToCsv(\n",
    "    input_dir  = txt_dir,\n",
    "    output_dir = csv_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "move the files the to correct folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(HMM_RES_DIR + hmm_level):\n",
    "    shutil.rmtree(HMM_RES_DIR + hmm_level)\n",
    "shutil.move('JPESTResults/selfTrained/' + hmm_level, HMM_RES_DIR)\n",
    "os.chdir('../LSTM Approach')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate hmm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mean: \n",
      "M_GEN:  0.5724991389949875\n",
      "M_HIGH:  0.6403457066119016\n",
      "M_SOFT:  0.8447943944896392\n"
     ]
    }
   ],
   "source": [
    "getScoresForHmm(test_files, hmm_res_files, hmm_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train DeepLearning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "3929/3929 [==============================] - 59s 14ms/step - loss: 1.1364 - categorical_accuracy: 0.5102\n",
      "\n",
      "Epoch 00001: saving model to trained_models/cp.ckpt\n",
      "Epoch 2/8\n",
      "3929/3929 [==============================] - 58s 15ms/step - loss: 0.9426 - categorical_accuracy: 0.6217\n",
      "\n",
      "Epoch 00002: saving model to trained_models/cp.ckpt\n",
      "Epoch 3/8\n",
      "3929/3929 [==============================] - 58s 15ms/step - loss: 0.7930 - categorical_accuracy: 0.6882\n",
      "\n",
      "Epoch 00003: saving model to trained_models/cp.ckpt\n",
      "Epoch 4/8\n",
      "3929/3929 [==============================] - 58s 15ms/step - loss: 0.6630 - categorical_accuracy: 0.7441\n",
      "\n",
      "Epoch 00004: saving model to trained_models/cp.ckpt\n",
      "Epoch 5/8\n",
      "3929/3929 [==============================] - 59s 15ms/step - loss: 0.5874 - categorical_accuracy: 0.7722\n",
      "\n",
      "Epoch 00005: saving model to trained_models/cp.ckpt\n",
      "Epoch 6/8\n",
      "3929/3929 [==============================] - 59s 15ms/step - loss: 0.5083 - categorical_accuracy: 0.7983\n",
      "\n",
      "Epoch 00006: saving model to trained_models/cp.ckpt\n",
      "Epoch 7/8\n",
      "3929/3929 [==============================] - 60s 15ms/step - loss: 0.4657 - categorical_accuracy: 0.8172\n",
      "\n",
      "Epoch 00007: saving model to trained_models/cp.ckpt\n",
      "Epoch 8/8\n",
      "3929/3929 [==============================] - 60s 15ms/step - loss: 0.4326 - categorical_accuracy: 0.8283\n",
      "\n",
      "Epoch 00008: saving model to trained_models/cp.ckpt\n"
     ]
    }
   ],
   "source": [
    "train_input_list, train_label_list = toVectorFutureTrainFormat(train_files, DATA_DIR)\n",
    "trainModel(train_input_list, train_label_list, num_epochs = 8, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval DeepLearning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = loadModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of mulitple fingering for a single piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.forward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.forward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.forward_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.backward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.backward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.backward_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.forward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.forward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.forward_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.backward_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.backward_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.backward_layer.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Total mean: \n",
      "M_GEN:  0.49459820647750596\n",
      "M_HIGH:  0.5382149169361405\n",
      "M_SOFT:  0.7576591694956264\n"
     ]
    }
   ],
   "source": [
    "getScoresForDL(test_files, hmm_res_files, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
